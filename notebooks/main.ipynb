{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yassine.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y-AiiAfKad6S",
        "CCS8J0rJ3XVW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxHn5IaVV19R",
        "colab_type": "text"
      },
      "source": [
        "# **I. Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILwiJk1YVp-O",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b7fb24c2-203a-4375-cf15-8809845f691a"
      },
      "source": [
        "#@title **I. 0. Import libraries**\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.linear_model import Lars\n",
        "from sklearn.linear_model import LassoLars\n",
        "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import ARDRegression\n",
        "from sklearn.linear_model import PassiveAggressiveRegressor\n",
        "from sklearn.linear_model import RANSACRegressor\n",
        "from sklearn.linear_model import TheilSenRegressor\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from xgboost import XGBRegressor\n",
        "#from catboost import CatBoostRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "import pandas.core.algorithms as algos\n",
        "from pandas import Series\n",
        "import scipy.stats.stats as stats\n",
        "import re\n",
        "import traceback\n",
        "import string\n",
        "import pdb\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCBGod69WRa5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "5ab5e728-9c07-42b2-aab2-4ffac9bd0b94"
      },
      "source": [
        "#@title **I. 1. Import data**\n",
        "BASE_PATH = \"/content/drive/My Drive/McK\" #@param\n",
        "#@markdown Name of imported variables:\n",
        "#@markdown - train (pd.DataFrame) : The table of train data\n",
        "#@markdown - test (pd.DataFrame) : The table of test data\n",
        "#@markdown - mapping_constellation (dict) : Mapping galaxy => constellation\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train = pd.read_csv(f\"{BASE_PATH}/train.csv\")\n",
        "test = pd.read_csv(f\"{BASE_PATH}/test.csv\")\n",
        "mapping_constellation = pd.read_excel(f\"{BASE_PATH}/mapping_galaxies.xlsx\")\n",
        "mapping_constellation = mapping_constellation.set_index(\"galaxy\").to_dict(orient=\"dict\")[\"constellation\"]\n",
        "print(\n",
        "    f\"Shape of train data {train.shape} \\n\"\n",
        "    f\"Shape of test data {test.shape}\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Shape of train data (3865, 80) \n",
            "Shape of test data (890, 79)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAmcMPmAXxXo",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title **I. 2. Evaluation function**\n",
        "#@markdown You can find here the different function to use for evaluation <br/>\n",
        "#@markdown The functions can be used in the following situations <br/>\n",
        "#@markdown - `RMSE(y_true, y_pred)` Get the RMSE of y_true and y_pred\n",
        "#@markdown - `train_and_evaluate(model, X, y)` Get all the scores of the 5-fold cross validation of your model\n",
        "#@markdown - `get_mean_score(model, X, y, verbose=True)` Get the mean and std of the scores of the 5-fold cross validation \n",
        "#@markdown - `compare_models(model, X, y, verbose=False)` Given a list of models, it compares them and return a DataFrame of the results\n",
        "\n",
        "def RMSE(y_true, y_pred):\n",
        "  return sqrt(mean_squared_error(y_true, y_pred))\n",
        "def train_and_evaluate(model, X, y):\n",
        "  kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "  scores = []\n",
        "  for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "    X_test, X_train, y_train = preprocess(X_test, X_train, y_train)\n",
        "    columns_here = [*set(X_train.columns).intersection(X_test.columns)]\n",
        "    model.fit(X_train[columns_here], y_train)\n",
        "    scores.append(RMSE(y_test, model.predict(X_test[columns_here])))\n",
        "  return scores\n",
        "\n",
        "def get_mean_score(model, X, y, verbose=True):\n",
        "  scores = train_and_evaluate(model, X, y)\n",
        "  mean = np.mean(scores)\n",
        "  std = np.std(scores)\n",
        "  if verbose:\n",
        "    print(\n",
        "      f\"5-Fold RMSE for the model {type(model).__name__}: \"\n",
        "      f\"{mean:.2e} +/- {std:.2e}\"\n",
        "    )\n",
        "    print(scores)\n",
        "  return mean, std\n",
        "\n",
        "def compare_models(model_list, X, y, verbose=False):\n",
        "  model_names = [type(model).__name__ for model in model_list]\n",
        "  df = pd.DataFrame(\n",
        "      np.nan,\n",
        "      index=model_names,\n",
        "      columns = [\"Mean RMSE\", \"Std RMSE\"]\n",
        "  )\n",
        "  for model in models:\n",
        "    model_name = type(model).__name__\n",
        "    try:\n",
        "      mean, std = get_mean_score(model, X, y, verbose=verbose)\n",
        "      df.loc[model_name, \"Mean RMSE\"] = mean\n",
        "      df.loc[model_name, \"Std RMSE\"] = std\n",
        "    except:\n",
        "      print(f\"The model {model_name} was skipped\")\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNgbgDWkWA4v",
        "colab_type": "text"
      },
      "source": [
        "# **II. Data Preprocesing**\n",
        "All data preprocessing on data should be done here (filtering, NaN droping and filling, cleaning functions...)\n",
        "1. For each cleaning or preprocessing, create a function\n",
        "2.  Then add it to the prprocess function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m3KhOdWLJP0",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title **Utils Functions and Constants**\n",
        "#@markdown Here you can store your utils functions. \n",
        "\n",
        "#@markdown - `compute_info_value` : compute information value of column against target\n",
        "#@markdown - `GENDER_COLS_COUPLES` : couples of male/female column\n",
        "\n",
        "GENDER_COLS_COUPLES = [\n",
        "    (\n",
        "      'Estimated_gross_galactic_income_per_capita_female',\n",
        "      'Estimated_gross_galactic_income_per_capita_male'\n",
        "    ),\n",
        "    (\n",
        "      'Expected_years_of_education_female_galactic_years',\n",
        "      'Expected_years_of_education_male_galactic_years',\n",
        "    ),\n",
        "    (\n",
        "      'Expected_years_of_education_female_galactic_years',\n",
        "      'Expected_years_of_education_male_galactic_years',\n",
        "    ),\n",
        "    (\n",
        "      'Intergalactic_Development_Index_IDI_female',\n",
        "      'Intergalactic_Development_Index_IDI_male',\n",
        "    ),\n",
        "    (\n",
        "      'Intergalactic_Development_Index_IDI_female_Rank',\n",
        "      'Intergalactic_Development_Index_IDI_male_Rank',\n",
        "    ),\n",
        "    (\n",
        "      'Labour_force_participation_rate__ages_15_and_older_female',\n",
        "      'Labour_force_participation_rate__ages_15_and_older_male',  \n",
        "    ),\n",
        "    (\n",
        "      'Labour_force_participation_rate__ages_15_and_older_female',\n",
        "      'Labour_force_participation_rate__ages_15_and_older_male',\n",
        "    ),\n",
        "    (\n",
        "      'Mean_years_of_education_female_galactic_years',\n",
        "      'Mean_years_of_education_male_galactic_years',\n",
        "    ),\n",
        "    (\n",
        "      'Population_with_at_least_some_secondary_education_female__ages_25_and_older',\n",
        "      'Population_with_at_least_some_secondary_education_male__ages_25_and_older'\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "max_bin = 20\n",
        "force_bin = 3\n",
        "\n",
        "def mono_bin(Y, X, n = max_bin):\n",
        "    \n",
        "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
        "    r = 0\n",
        "    while np.abs(r) < 1:\n",
        "        try:\n",
        "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
        "            d2 = d1.groupby('Bucket', as_index=True)\n",
        "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
        "            n = n - 1 \n",
        "        except Exception as e:\n",
        "            n = n - 1\n",
        "\n",
        "    if len(d2) == 1:\n",
        "        n = force_bin         \n",
        "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
        "        if len(np.unique(bins)) == 2:\n",
        "            bins = np.insert(bins, 0, 1)\n",
        "            bins[1] = bins[1]-(bins[1]/2)\n",
        "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
        "        d2 = d1.groupby('Bucket', as_index=True)\n",
        "    \n",
        "    d3 = pd.DataFrame({},index=[])\n",
        "    d3[\"MIN_VALUE\"] = d2.min().X\n",
        "    d3[\"MAX_VALUE\"] = d2.max().X\n",
        "    d3[\"COUNT\"] = d2.count().Y\n",
        "    d3[\"EVENT\"] = d2.sum().Y\n",
        "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
        "    d3=d3.reset_index(drop=True)\n",
        "    \n",
        "    if len(justmiss.index) > 0:\n",
        "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "        d4[\"MAX_VALUE\"] = np.nan\n",
        "        d4[\"COUNT\"] = justmiss.count().Y\n",
        "        d4[\"EVENT\"] = justmiss.sum().Y\n",
        "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "        d3 = d3.append(d4,ignore_index=True)\n",
        "    \n",
        "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"VAR_NAME\"] = \"VAR\"\n",
        "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n",
        "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "    d3.IV = d3.IV.sum()\n",
        "    \n",
        "    return(d3)\n",
        "\n",
        "def char_bin(Y, X):\n",
        "        \n",
        "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "    notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
        "    df2 = notmiss.groupby('X',as_index=True)\n",
        "    \n",
        "    d3 = pd.DataFrame({},index=[])\n",
        "    d3[\"COUNT\"] = df2.count().Y\n",
        "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
        "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
        "    d3[\"EVENT\"] = df2.sum().Y\n",
        "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
        "    \n",
        "    if len(justmiss.index) > 0:\n",
        "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "        d4[\"MAX_VALUE\"] = np.nan\n",
        "        d4[\"COUNT\"] = justmiss.count().Y\n",
        "        d4[\"EVENT\"] = justmiss.sum().Y\n",
        "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "        d3 = d3.append(d4,ignore_index=True)\n",
        "    \n",
        "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"VAR_NAME\"] = \"VAR\"\n",
        "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
        "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "    d3.IV = d3.IV.sum()\n",
        "    d3 = d3.reset_index(drop=True)\n",
        "    \n",
        "    return(d3)\n",
        "\n",
        "def compute_info_value(df1, target):\n",
        "    \n",
        "    stack = traceback.extract_stack()\n",
        "    filename, lineno, function_name, code = stack[-2]\n",
        "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
        "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
        "    \n",
        "    x = df1.dtypes.index\n",
        "    count = -1\n",
        "    \n",
        "    for i in x:\n",
        "        if i.upper() not in (final.upper()):\n",
        "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
        "                conv = mono_bin(target, df1[i])\n",
        "                conv[\"VAR_NAME\"] = i\n",
        "                count = count + 1\n",
        "            else:\n",
        "                conv = char_bin(target, df1[i])\n",
        "                conv[\"VAR_NAME\"] = i            \n",
        "                count = count + 1\n",
        "                \n",
        "            if count == 0:\n",
        "                iv_df = conv\n",
        "            else:\n",
        "                iv_df = iv_df.append(conv,ignore_index=True)\n",
        "    \n",
        "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
        "    iv = iv.reset_index()\n",
        "    return(iv_df,iv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhaugOXgVzHF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title **Preprocessing Functions**\n",
        "#@markdown Here you can create your preprocssing functions\n",
        "\n",
        "def clean_column_names(data):\n",
        "  columns = data.columns\n",
        "  data.columns = [\n",
        "                        (\"_\".join(\n",
        "      col.encode(\"ascii\", errors=\"ignore\")\n",
        "      .decode()\n",
        "      .split(\" \"))\n",
        "      .replace(\",\",\"\")\n",
        "      .replace(\"(\",\"\")\n",
        "      .replace(\"[\",\"\")\n",
        "      .replace(\"]\",\"\")\n",
        "      .replace(\")\",\"\")\n",
        "      .replace(\"%\",\"\")\n",
        "  ) for col in data.columns\n",
        "  ]\n",
        "  return data\n",
        "\n",
        "def remove_year(data, train_data):\n",
        "  data = data.drop(columns=[\"galactic_year\"])\n",
        "  return data\n",
        "\n",
        "def remove_constellation_and_year(data, train_data):\n",
        "  data = data.drop(columns=[\"galaxy\", \"galactic_year\"])\n",
        "  return data\n",
        "\n",
        "def fill_na(data, train_data):\n",
        "  mean = data.mean()\n",
        "  data = data.fillna(mean)\n",
        "  return data\n",
        "\n",
        "def create_na_columns(data, train_data):\n",
        "  for col in data.columns:\n",
        "    if data[col].isna().any():\n",
        "      data[f\"{col}_isna\"] = (data[col].isna()).astype(\"int\")\n",
        "  return data\n",
        "\n",
        "def drop_sparse(data, train_data):\n",
        "  na_mean = train_data.isna().mean()\n",
        "  is_sparse = na_mean[na_mean > 0.5]\n",
        "  data = data.drop(is_sparse.index.values, axis=1)\n",
        "  return data\n",
        "\n",
        "def dummify_galaxy(data, train_data):\n",
        "  data = pd.get_dummies(data)\n",
        "  return data\n",
        "\n",
        "def label_encode_galaxy(data, data_train):\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  le.fit(data_train[\"galaxy\"])\n",
        "  data[\"galaxy\"] = le.transform(data[\"galaxy\"])\n",
        "  return data\n",
        "\n",
        "def one_hot_galaxy(data, data_train):\n",
        "  data = pd.get_dummies(data)\n",
        "  galaxies_not_in_test = ['Andromeda XII','Andromeda XIX[60]','Andromeda XVIII[60]','Andromeda XXII[57]','Andromeda XXIV','Hercules Dwarf','NGC 5253','Triangulum Galaxy (M33)',\"Tucana Dwarf\"]\n",
        "  galaxies_not_in_test = [f\"galaxy_{col}\" for col in galaxies_not_in_test]\n",
        "  data[\"galaxy_not_in_test\"] = 0\n",
        "  if set(galaxies_not_in_test).issubset(data.columns):\n",
        "    data[\"galaxy_not_in_test\"] = (\n",
        "        data[galaxies_not_in_test].sum(axis=1) > 0\n",
        "    ).astype(\"int\")\n",
        "    data = data.drop([*galaxies_not_in_test], axis=1)\n",
        "  return data\n",
        "\n",
        "def create_period_column(data, train_data):\n",
        "  start = data['galactic_year'].min()\n",
        "  end = data['galactic_year'].max()\n",
        "  pas = 2510\n",
        "  m=start \n",
        "  Period = [start]\n",
        "  while m < end :\n",
        "    m+=pas\n",
        "    Period.append(m)\n",
        "\n",
        "  def get_period(x):\n",
        "    p=0\n",
        "    while x>=Period[p]:\n",
        "      p+=1\n",
        "    return(p)\n",
        "  data[\"period\"] = data[\"galactic_year\"].apply(get_period)\n",
        "\n",
        "  return data\n",
        "\n",
        "def fill_na_period_galaxy(data, train_data):\n",
        "  data = data.fillna(data.groupby(['galaxy','period']).transform('mean'))\n",
        "  data = data.fillna(data.groupby(['period']).transform('mean'))\n",
        "  data = data.fillna(data.groupby(['galaxy']).transform('mean'))\n",
        "  data = data.fillna(data.mean())\n",
        "  return data\n",
        "\n",
        "def compute_dwarf_planet(data, train_data):\n",
        "  data[\"dwarf_planet\"] = (\n",
        "      data[\"galaxy\"]\n",
        "      .str.lower()\n",
        "      .str.contains(\"dwarf\")\n",
        "      .astype(\"int\")\n",
        "  )\n",
        "  return data\n",
        "\n",
        "def compute_constellation(data, train_data):\n",
        "  data[\"constellation\"] = data[\"galaxy\"].replace(mapping_constellation)\n",
        "  return data\n",
        "\n",
        "def one_hot_constellation(data, data_train):\n",
        "  data = pd.get_dummies(data)\n",
        "  constellations_not_in_test = [\"constellation_Hercules\"]\n",
        "  data[\"constellation_not_in_test\"] = 0\n",
        "  if set(constellations_not_in_test).issubset(data.columns):\n",
        "    data[\"constellation_not_in_test\"] = (\n",
        "        data[constellations_not_in_test].sum(axis=1) > 0\n",
        "    ).astype(\"int\")\n",
        "    data = data.drop([*constellations_not_in_test], axis=1)\n",
        "  return data\n",
        "\n",
        "def one_hot_galaxy_constellation(data, data_train):\n",
        "  data = pd.get_dummies(data)\n",
        "  constellations_not_in_test = [\"constellation_Hercules\"]\n",
        "  data[\"constellation_not_in_test\"] = 0\n",
        "  if set(constellations_not_in_test).issubset(data.columns):\n",
        "    data[\"constellation_not_in_test\"] = (\n",
        "        data[constellations_not_in_test].sum(axis=1) > 0\n",
        "    ).astype(\"int\")\n",
        "    data = data.drop([*constellations_not_in_test], axis=1)\n",
        "  galaxies_not_in_test = ['Andromeda XII','Andromeda XIX[60]','Andromeda XVIII[60]','Andromeda XXII[57]','Andromeda XXIV','Hercules Dwarf','NGC 5253','Triangulum Galaxy (M33)',\"Tucana Dwarf\"]\n",
        "  galaxies_not_in_test = [f\"galaxy_{col}\" for col in galaxies_not_in_test]\n",
        "  data[\"galaxy_not_in_test\"] = 0\n",
        "  if set(galaxies_not_in_test).issubset(data.columns):\n",
        "    data[\"galaxy_not_in_test\"] = (\n",
        "        data[galaxies_not_in_test].sum(axis=1) > 0\n",
        "    ).astype(\"int\")\n",
        "    data = data.drop([*galaxies_not_in_test], axis=1)\n",
        "  return data\n",
        "\n",
        "\n",
        "def remove_weight_of_evidence(data, train_data):\n",
        "  final_iv, IV = compute_info_value(train_data,train_data[\"y\"])\n",
        "  to_drop = IV[IV['IV'] < 0.002]\n",
        "  to_drop = to_drop.VAR_NAME.to_list()\n",
        "  data = data.drop(to_drop, axis = 1)\n",
        "  return data\n",
        "\n",
        "def log_scale_data(data, train_data):\n",
        "  col_log = data.select_dtypes(\"float64\").columns\n",
        "  data[col_log] = data[col_log].abs().apply(lambda x: np.log(1+x))\n",
        "  return data\n",
        "\n",
        "def get_gender_ratio(data, train_data, gender_columns_couples=GENDER_COLS_COUPLES):\n",
        "  for (male, female) in gender_columns_couples:\n",
        "    new_col_name = female.replace(\"_female\",\"_gender_ration\")\n",
        "    data[new_col_name] = data[female]/data[male]\n",
        "  return data\n",
        "\n",
        "def remove_negative_values(data, train_data):\n",
        "  data = data[(data >= 0).all(1)]\n",
        "  return data\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENdmfsmiWurj",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title **The Function preprocess**\n",
        "#@markdown Here you can add the created functions to the function `preprocess`\n",
        "\n",
        "def preprocess(X_test, X_train, y_train, target=\"y\"):\n",
        "  \"\"\"This method will be deployed on data to do the preprocesing\n",
        "\n",
        "   Args:\n",
        "    data (pd.DataFrame): The data on which the preprocessing is applied\n",
        "    train_data (pd.DataFrame): The train data used to train encoders\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: The preprocessed data\n",
        "  \"\"\"\n",
        "  data = X_test.copy()\n",
        "  train_data = pd.concat([X_train, y_train], axis=1)\n",
        "  data, train_data = clean_column_names(data), clean_column_names(train_data)\n",
        "  data, train_data = get_gender_ratio(data, train_data), get_gender_ratio(train_data, train_data)\n",
        "  data, train_data = drop_sparse(data, train_data), drop_sparse(train_data, train_data)\n",
        "  data, train_data = create_period_column(data, train_data), create_period_column(train_data, train_data)\n",
        "  data, train_data = create_na_columns(data, train_data), create_na_columns(train_data, train_data)\n",
        "  data, train_data = fill_na_period_galaxy(data, train_data), fill_na_period_galaxy(train_data, train_data)\n",
        "  data, train_data = remove_year(data, train_data), remove_year(train_data, train_data)\n",
        "  data, train_data = compute_dwarf_planet(data, train_data), compute_dwarf_planet(train_data, train_data)\n",
        "  data, train_data = compute_constellation(data, train_data), compute_constellation(train_data, train_data)\n",
        "  data, train_data = one_hot_galaxy_constellation(data, train_data), one_hot_galaxy_constellation(train_data, train_data)\n",
        "  data, train_data = data, remove_negative_values(train_data, train_data)\n",
        "  data, train_data = clean_column_names(data), clean_column_names(train_data)\n",
        "  \n",
        "  return data, train_data.drop(target, axis=1), train_data[target]\n",
        "\n",
        "def train_preprocessing(data, target=\"y\"):\n",
        "  _, X, y = preprocess(data, data, target)\n",
        "  X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySCsMSZCaKjh",
        "colab_type": "text"
      },
      "source": [
        "# **IV. Model**\n",
        "Train and evaluate the model(s) here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkOQnpMWeApM",
        "colab_type": "text"
      },
      "source": [
        "### **IV. 1. Cross validation**\n",
        "Here we can compare models by doing 5-fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGhFSxwYjKt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBsNMtL4O8uM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_to_eval = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0, criterion='mae',\n",
        "                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "                    max_samples=None, min_impurity_decrease=0.0,\n",
        "                    min_impurity_split=None, min_samples_leaf=1,\n",
        "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                    n_estimators=10, n_jobs=None, oob_score=False,\n",
        "                    random_state=None, verbose=0, warm_start=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MA8u0y6Dnu_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba578828-876a-4e44-ed9d-d6e647e1007e"
      },
      "source": [
        "#@title **Train Test Validation**\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    train.drop(\"y\", axis=1), train[[\"y\"]], test_size=0.33\n",
        ")\n",
        "X_test_preprocessed, X_train_p, y_train_p = preprocess(X_test,X_train,y_train)\n",
        "columns_here = [*set(X_train_p.columns).intersection(X_test_preprocessed.columns)]\n",
        "model_to_eval.fit(X_train_p[columns_here], y_train_p)\n",
        "y_hat = model_to_eval.predict(X_test_preprocessed[columns_here])\n",
        "RMSE(y_test, y_hat)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04082782713161926"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnfpqg9ziJH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d248b4da-d943-4bdc-8d64-7b515cc65d80"
      },
      "source": [
        "from sklearn.linear_model import OrthogonalMatchingPursuit, ARDRegression, PoissonRegressor, TweedieRegressor\n",
        "model_to_eval = PoissonRegressor()\n",
        "model_to_eval.fit(X_train_p[columns_here], y_train_p)\n",
        "y_hat = model_to_eval.predict(X_test_preprocessed[columns_here])\n",
        "RMSE(y_test, y_hat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.020438388999207482"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sdwSkHSJaAR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "4e7faf5d-0371-4ef6-e1d1-447601a2f0fa"
      },
      "source": [
        "#@title **5-fold Cross Validation**\n",
        "get_mean_score(\n",
        "    model_to_eval, \n",
        "    train.drop(\"y\", axis=1), \n",
        "    train[[\"y\"]]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-ea12a46b74b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_to_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-3-9454f46ea0eb>\u001b[0m in \u001b[0;36mget_mean_score\u001b[0;34m(model, X, y, verbose)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_mean_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9454f46ea0eb>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcolumns_here\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns_here\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns_here\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 383\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v94LU6tdW37w",
        "colab_type": "text"
      },
      "source": [
        "### **IV. 2. Fine Tuning**\n",
        "Here we can compare models by doing 5-fold cross validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYFVTpVEW5T3",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title **Utils Functions**\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models(parameter):\n",
        "  models = dict()\n",
        "  if parameter == \"n_estimators\":\n",
        "    models['10'] = ExtraTreesRegressor(n_estimators=10)\n",
        "    models['50'] = ExtraTreesRegressor(n_estimators=50)\n",
        "    models['100'] = ExtraTreesRegressor(n_estimators=100)\n",
        "    models['500'] = ExtraTreesRegressor(n_estimators=500)\n",
        "    models['1000'] = ExtraTreesRegressor(n_estimators=1000)\n",
        "    models['5000'] = ExtraTreesRegressor(n_estimators=5000)\n",
        "  elif parameter == \"max_features\":\n",
        "    for i in range(1, 21):\n",
        "      models[str(i)] = ExtraTreesRegressor(max_features=i)\n",
        "  elif parameter == \"min_samples_split\":\n",
        "    for i in range(2, 15):\n",
        "      models[str(i)] = ExtraTreesRegressor(min_samples_split=i)\n",
        "  \n",
        "  return models\n",
        " \n",
        "# evaluate a given model using cross-validation\n",
        "def evaluate_model(model, data):\n",
        "\tscores = train_and_evaluate(model,  data.drop(\"y\", axis=1), data[[\"y\"]])\n",
        "\treturn scores\n",
        " \n",
        "# get the models to evaluate\n",
        "def plot_results(parameter:str):\n",
        "  models = get_models(parameter)\n",
        "  # evaluate the models and store results\n",
        "  results, names = list(), list()\n",
        "  for name, model in models.items():\n",
        "    scores = evaluate_model(model, train)\n",
        "    results.append(scores)\n",
        "    names.append(name)\n",
        "    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
        "  # plot model performance for comparison\n",
        "  pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "  pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR-uiKdMZbZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_results(\"max_features\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G87YTahZeRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_results(\"min_samples_split\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbqshClNeHNp",
        "colab_type": "text"
      },
      "source": [
        "### **IV. 3. Train chosen model**\n",
        "Here we train the chosen model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhP2APhOeOuR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4fc02a6d-82c8-4f7c-80c0-e0930bf5bb8f"
      },
      "source": [
        "model = ExtraTreesRegressor() #@param\n",
        "model.fit(X, y)\n",
        "#@markdown Output model listed as the variable `model`"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
              "                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                    max_samples=None, min_impurity_decrease=0.0,\n",
              "                    min_impurity_split=None, min_samples_leaf=1,\n",
              "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                    n_estimators=100, n_jobs=None, oob_score=False,\n",
              "                    random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-AiiAfKad6S",
        "colab_type": "text"
      },
      "source": [
        "# **V. Deployment on test data**\n",
        "Here you can deploy your model on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pip3uouzak_r",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title **Make predictions**\n",
        "#@markdown The predictions are stored in the variable `y_pred`\n",
        "X_pred = preprocess(test, train)\n",
        "y_pred = model.predict(X_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LdMZTF5ZorT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title **Save predictions**\n",
        "#@markdown Please make sure you create the directory **Predictions** in your **BASE_PATH**\n",
        "time = str(datetime.now())\n",
        "now = time[:16].replace(\" \", \"_\").replace(\"-\",\"_\").replace(\":\",\"h\")\n",
        "#@markdown Enter a filename\n",
        "#@markdown > If you don't choose a filename, the predictions are saved with a timestamp in the directory **Predictions** of your **BASE_PATH**.\n",
        "\n",
        "filename = \"\" #@param {type:\"string\"}\n",
        "if filename == \"\":\n",
        "  filename = f\"{BASE_PATH}/Predictions/predictions_{now}\"\n",
        "pd.Series(y_pred).to_csv(f\"{filename}.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CCS8J0rJ3XVW"
      },
      "source": [
        "# **VI. Optimization**\n",
        "Here you can create your optimization model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2EZQNAQ3c8m",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3800e6e2-a5eb-437b-827d-540f92692c24"
      },
      "source": [
        "#@title **Create optimization dataframe**\n",
        "#@markdown - Create `opt_df` a dataframe from the prediction\n",
        "#@markdown - Create the column `potential_increase` in `opt_df` using the formula `-np.log(Index+0.01)+3`\n",
        "\n",
        "opt_df = pd.DataFrame(y_pred, columns=[\"pred\"]).reset_index()\n",
        "opt_df[\"potential_increase\"] = -np.log(opt_df.pred + 0.01) + 3\n",
        "opt_df[\"existence_expectancy_index\"] = test[\"existence_expectancy_index\"]\n",
        "opt_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>pred</th>\n",
              "      <th>potential_increase</th>\n",
              "      <th>existence_expectancy_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.046765</td>\n",
              "      <td>5.868832</td>\n",
              "      <td>0.456086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.045294</td>\n",
              "      <td>5.895096</td>\n",
              "      <td>0.529835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.045975</td>\n",
              "      <td>5.882844</td>\n",
              "      <td>0.560976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.046946</td>\n",
              "      <td>5.865653</td>\n",
              "      <td>0.565910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.045284</td>\n",
              "      <td>5.895277</td>\n",
              "      <td>0.588274</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index      pred  potential_increase  existence_expectancy_index\n",
              "0      0  0.046765            5.868832                    0.456086\n",
              "1      1  0.045294            5.895096                    0.529835\n",
              "2      2  0.045975            5.882844                    0.560976\n",
              "3      3  0.046946            5.865653                    0.565910\n",
              "4      4  0.045284            5.895277                    0.588274"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wcVyjC25K4Y",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title **Optimization parameters**\n",
        "\n",
        "TOTAL_ENERGY =  50000#@param {type:\"number\"}\n",
        "MAX_ENERGY_PER_GALAXY = 100 #@param {type:\"number\"}\n",
        "THRESHOLD = 0.7 #@param {type:\"number\"}\n",
        "MINIMAL_ENERGY_UNDER_THRESHOLD = 0.1 #@param {type:\"number\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-tdJd6C7ZcY",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title **Cost Function and Constraints**\n",
        "#@markdown We want to maximize `extra_energy * (potential_increase**2) / 1000` , i.e. minimize the cost function ` - extra_energy * (potential_increase**2) / 1000`\n",
        "\n",
        "def cost_function(extra_energy, potential_increase=opt_df.potential_increase.values, pred=opt_df.pred):\n",
        "  cost = np.sum(- extra_energy * (potential_increase**2) / 1000)\n",
        "  return cost\n",
        "  \n",
        "def under_threshold(\n",
        "    extra_energy, \n",
        "    existency_index=opt_df.existence_expectancy_index\n",
        "):\n",
        "  here = pd.DataFrame(\n",
        "      {\n",
        "          \"extra_energy\":extra_energy, \n",
        "          \"existency_index\":existency_index,\n",
        "       }\n",
        "  )\n",
        "  sum_ = here[here.existency_index<=THRESHOLD][\"extra_energy\"].sum()\n",
        "  return sum_ - MINIMAL_ENERGY_UNDER_THRESHOLD*TOTAL_ENERGY\n",
        "\n",
        "def total_energy(extra_energy):\n",
        "  sum_ = np.sum(extra_energy)\n",
        "  return TOTAL_ENERGY - sum_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a4VcNUpPbbs",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title **Optimization**\n",
        "#@markdown Here we minimize the cost function subject to the constraints\n",
        "from scipy import optimize\n",
        "x0 = np.array([0]*len(opt_df))\n",
        "result = optimize.minimize(\n",
        "    cost_function, \n",
        "    x0, \n",
        "    method=\"SLSQP\",\n",
        "    bounds= [(0, MAX_ENERGY_PER_GALAXY) for i in range(len(opt_df))],\n",
        "    constraints = [\n",
        "      {\"fun\": total_energy, \"type\":\"ineq\"},\n",
        "      {\"fun\": under_threshold, \"type\":\"ineq\"}\n",
        "    ],\n",
        ")\n",
        "\n",
        "opt_df[\"pred_opt\"] = result[\"x\"]\n",
        "opt_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJxuzbOaigea",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title **Save final results**\n",
        "final_df = opt_df[[\"index\",\"pred\", \"pred_opt\"]]\n",
        "final_df.columns = [\"index\",\"pred\", \"opt_pred\"]\n",
        "#@markdown Please make sure you create the directory **Predictions** in your **BASE_PATH**\n",
        "time = str(datetime.now())\n",
        "now = time[:16].replace(\" \", \"_\").replace(\"-\",\"_\").replace(\":\",\"h\")\n",
        "#@markdown Enter a filename\n",
        "#@markdown > If you don't choose a filename, the predictions are saved with a timestamp in the directory **Predictions** of your **BASE_PATH**.\n",
        "\n",
        "filename = \"\" #@param {type:\"string\"}\n",
        "if filename == \"\":\n",
        "  filename = f\"{BASE_PATH}/Predictions/output_{now}\"\n",
        "final_df.to_csv(f\"{filename}.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}